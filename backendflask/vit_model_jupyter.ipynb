{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "929cab58-76d7-4506-89d9-9f3138acc647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported and seeds set!\n"
     ]
    }
   ],
   "source": [
    "# üõ†Ô∏è Install missing packages if any\n",
    "# !pip install -q torch torchvision tqdm\n",
    "\n",
    "# üì¶ Imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# üßπ Set random seeds for reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "print(\"‚úÖ Libraries imported and seeds set!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "382ae0cd-3409-4de3-bd34-d4cf9fa838e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evidence classes and label encoder ready!\n"
     ]
    }
   ],
   "source": [
    "# üéØ Evidence classes\n",
    "evidence_classes = [\n",
    "    \"Gun\", \"Knife\", \"Mask\", \"Car\", \"Fire\", \"Glass\", \"Crowd\",\n",
    "    \"Blood\", \"Explosion\", \"Bag\", \"Money\", \"Weapon\", \"Smoke\", \"Person\"\n",
    "]\n",
    "\n",
    "# üîó Crime-to-evidence mapping\n",
    "crime_to_evidence = {\n",
    "    \"Arrest\": [\"Person\", \"Crowd\"],\n",
    "    \"Arson\": [\"Fire\", \"Smoke\"],\n",
    "    \"Assault\": [\"Blood\", \"Person\"],\n",
    "    \"Burglary\": [\"Bag\", \"Glass\"],\n",
    "    \"Explosion\": [\"Explosion\", \"Smoke\"],\n",
    "    \"Fighting\": [\"Crowd\", \"Blood\"],\n",
    "    \"RoadAccidents\": [\"Car\", \"Person\"],\n",
    "    \"Robbery\": [\"Gun\", \"Bag\", \"Mask\"],\n",
    "    \"Shooting\": [\"Gun\", \"Crowd\"],\n",
    "    \"Shoplifting\": [\"Bag\", \"Money\"],\n",
    "    \"Stealing\": [\"Bag\", \"Person\"],\n",
    "    \"Vandalism\": [\"Glass\", \"Crowd\"],\n",
    "    \"Abuse\": [\"Person\"],\n",
    "    \"NormalVideos\": []\n",
    "}\n",
    "\n",
    "# üè∑Ô∏è Encode function\n",
    "def encode_evidence_labels(crime_label_idx):\n",
    "    index_to_class = {\n",
    "        0: \"Arrest\", 1: \"Arson\", 2: \"Assault\", 3: \"Burglary\", 4: \"Explosion\",\n",
    "        5: \"Fighting\", 6: \"RoadAccidents\", 7: \"Robbery\", 8: \"Shooting\",\n",
    "        9: \"Shoplifting\", 10: \"Stealing\", 11: \"Vandalism\", 12: \"Abuse\", 13: \"NormalVideos\"\n",
    "    }\n",
    "    crime_class = index_to_class[crime_label_idx]\n",
    "    evidences = crime_to_evidence.get(crime_class, [])\n",
    "    binary = [1 if ev in evidences else 0 for ev in evidence_classes]\n",
    "    return torch.tensor(binary, dtype=torch.float32)\n",
    "\n",
    "print(\"‚úÖ Evidence classes and label encoder ready!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119b348f-411f-4423-a617-c770487208a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vision Transformer model defined!\n"
     ]
    }
   ],
   "source": [
    "# üß† Vision Transformer (ViT) for Evidence Detection\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=14, dim=256, depth=6, heads=8, mlp_dim=512, patch_size=8, image_size=64):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.dim = dim\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.conv = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, dim))\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, batch_first=True),\n",
    "            num_layers=depth\n",
    "        )\n",
    "\n",
    "        self.to_evidence = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)                  # (B, D, 8, 8)\n",
    "        x = x.flatten(2).transpose(1, 2)   # (B, 64, D)\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)  # (B, 1, D)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)                   # (B, 65, D)\n",
    "        x = x + self.pos_embedding\n",
    "        x = self.transformer(x)\n",
    "        x = x[:, 0]  # CLS token output\n",
    "        return self.to_evidence(x)\n",
    "\n",
    "print(\"‚úÖ Vision Transformer model defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a87e1a2c-a3d2-4722-9930-8941b9fee127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up data transformations...\n",
      "‚úÖ Transformations set!\n",
      "‚öôÔ∏è Batch Size: 128, Number of Classes: 14\n",
      "üî• Device selected: cuda\n",
      "üìÇ Loading UCF Crime dataset...\n",
      "üìä Dataset contains 1266345 samples across 14 classes.\n",
      "üìö Classes detected: ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fighting', 'NormalVideos', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']\n",
      "‚úÖ DataLoader is ready!\n",
      "üöÄ Everything ready to start training!\n"
     ]
    }
   ],
   "source": [
    "# üìÇ Define Transformations\n",
    "print(\"üîß Setting up data transformations...\")\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "print(\"‚úÖ Transformations set!\")\n",
    "\n",
    "# üî• Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = 14\n",
    "print(f\"‚öôÔ∏è Batch Size: {BATCH_SIZE}, Number of Classes: {NUM_CLASSES}\")\n",
    "\n",
    "# üöÄ Set Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üî• Device selected: {device}\")\n",
    "\n",
    "# üóÇÔ∏è Load Dataset\n",
    "print(\"üìÇ Loading UCF Crime dataset...\")\n",
    "train_dataset = datasets.ImageFolder(r\"C:/Users/adity/Downloads/Train\", transform=train_transform)\n",
    "\n",
    "print(f\"üìä Dataset contains {len(train_dataset)} samples across {len(train_dataset.classes)} classes.\")\n",
    "print(f\"üìö Classes detected: {train_dataset.classes}\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,   # Kaggle usually gives 2 CPUs\n",
    "    pin_memory=True\n",
    ")\n",
    "print(\"‚úÖ DataLoader is ready!\")\n",
    "print(\"üöÄ Everything ready to start training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e690292e-516e-42d3-852a-63332c932148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model, Loss, Optimizer ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_24512\\2226223630.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# üß† Model\n",
    "model = VisionTransformer(num_classes=NUM_CLASSES)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"üöÄ Using {torch.cuda.device_count()} GPUs with DataParallel!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# üéØ Loss and Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "print(\"‚úÖ Model, Loss, Optimizer ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d576468d-2b58-41f3-9e71-8db7d26e0abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Split: 1139711 training samples and 126634 validation samples.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# üõ†Ô∏è Split into train and validation\n",
    "VAL_SPLIT = 0.1  # 10% for validation\n",
    "\n",
    "# Calculate lengths\n",
    "num_samples = len(train_dataset)\n",
    "num_val = int(VAL_SPLIT * num_samples)\n",
    "num_train = num_samples - num_val\n",
    "\n",
    "# Random split\n",
    "train_dataset, val_dataset = random_split(train_dataset, [num_train, num_val])\n",
    "print(f\"üß© Split: {num_train} training samples and {num_val} validation samples.\")\n",
    "\n",
    "# Reload loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03e65d8c-6357-4355-b49a-c7b21c1ea826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] (Training): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8904/8904 [15:58<00:00,  9.29it/s, loss=0.00374]\n",
      "Epoch [1/10] (Validation): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 990/990 [01:52<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch [1/10] - Train Loss: 0.0109 | Train Acc: 99.64%\n",
      "üß™ Epoch [1/10] - Val Loss: 0.0052 | Val Acc: 99.83%\n",
      "üíæ Validation Loss improved from 0.0352 to 0.0052 - saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] (Training): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8904/8904 [15:09<00:00,  9.79it/s, loss=0.000848]\n",
      "Epoch [2/10] (Validation): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 990/990 [01:46<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch [2/10] - Train Loss: 0.0070 | Train Acc: 99.76%\n",
      "üß™ Epoch [2/10] - Val Loss: 0.0036 | Val Acc: 99.88%\n",
      "üíæ Validation Loss improved from 0.0052 to 0.0036 - saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] (Training): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8904/8904 [15:35<00:00,  9.52it/s, loss=0.00411]\n",
      "Epoch [3/10] (Validation): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 990/990 [01:59<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch [3/10] - Train Loss: 0.0053 | Train Acc: 99.82%\n",
      "üß™ Epoch [3/10] - Val Loss: 0.0029 | Val Acc: 99.90%\n",
      "üíæ Validation Loss improved from 0.0036 to 0.0029 - saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] (Training): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8904/8904 [14:18<00:00, 10.38it/s, loss=0.00117]\n",
      "Epoch [4/10] (Validation): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 990/990 [01:36<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch [4/10] - Train Loss: 0.0045 | Train Acc: 99.85%\n",
      "üß™ Epoch [4/10] - Val Loss: 0.0035 | Val Acc: 99.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] (Training): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8904/8904 [15:09<00:00,  9.79it/s, loss=0.000604]\n",
      "Epoch [5/10] (Validation): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 990/990 [01:57<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch [5/10] - Train Loss: 0.0039 | Train Acc: 99.87%\n",
      "üß™ Epoch [5/10] - Val Loss: 0.0026 | Val Acc: 99.91%\n",
      "üíæ Validation Loss improved from 0.0029 to 0.0026 - saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] (Training): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8904/8904 [16:55<00:00,  8.77it/s, loss=0.00019]\n",
      "Epoch [6/10] (Validation): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 990/990 [02:01<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch [6/10] - Train Loss: 0.0035 | Train Acc: 99.88%\n",
      "üß™ Epoch [6/10] - Val Loss: 0.0023 | Val Acc: 99.92%\n",
      "üíæ Validation Loss improved from 0.0026 to 0.0023 - saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] (Training): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8904/8904 [17:29<00:00,  8.49it/s, loss=0.000701]\n",
      "Epoch [7/10] (Validation): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 990/990 [01:57<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch [7/10] - Train Loss: 0.0032 | Train Acc: 99.89%\n",
      "üß™ Epoch [7/10] - Val Loss: 0.0022 | Val Acc: 99.93%\n",
      "üíæ Validation Loss improved from 0.0023 to 0.0022 - saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] (Training): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8904/8904 [16:26<00:00,  9.03it/s, loss=0.00135]\n",
      "Epoch [8/10] (Validation): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 990/990 [01:55<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch [8/10] - Train Loss: 0.0029 | Train Acc: 99.90%\n",
      "üß™ Epoch [8/10] - Val Loss: 0.0019 | Val Acc: 99.94%\n",
      "üíæ Validation Loss improved from 0.0022 to 0.0019 - saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] (Training): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8904/8904 [16:10<00:00,  9.17it/s, loss=0.000128]\n",
      "Epoch [9/10] (Validation): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 990/990 [01:53<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch [9/10] - Train Loss: 0.0028 | Train Acc: 99.90%\n",
      "üß™ Epoch [9/10] - Val Loss: 0.0022 | Val Acc: 99.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] (Training): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8904/8904 [16:34<00:00,  8.95it/s, loss=0.00612]\n",
      "Epoch [10/10] (Validation): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 990/990 [01:57<00:00,  8.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch [10/10] - Train Loss: 0.0026 | Train Acc: 99.91%\n",
      "üß™ Epoch [10/10] - Val Loss: 0.0020 | Val Acc: 99.94%\n",
      "\n",
      "üéØ Training completed!\n",
      "üèÜ Best model saved at: vit_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "\n",
    "# Ensure optimizer is linked to model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{EPOCHS}] (Training)\", leave=True)\n",
    "\n",
    "    for images, labels in loop:\n",
    "        images = images.to(device)\n",
    "        multi_labels = torch.stack([encode_evidence_labels(lbl.item()) for lbl in labels]).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, multi_labels)\n",
    "\n",
    "        # Backpropagate gradients with scaled loss\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        try:\n",
    "            scaler.step(optimizer)  # Step optimizer only if gradients are not missing\n",
    "            scaler.update()\n",
    "        except AssertionError:\n",
    "            print(\"‚ö†Ô∏è Skipping optimizer step due to missing gradients.\")\n",
    "            continue  # Skip this batch if gradients are missing\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # üéØ Calculate batch accuracy\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        correct += (preds == multi_labels.bool()).sum().item()\n",
    "        total += torch.numel(multi_labels)\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Average loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "\n",
    "    # üß™ Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_total = 0\n",
    "    val_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loop = tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{EPOCHS}] (Validation)\", leave=True)\n",
    "\n",
    "        for images, labels in val_loop:\n",
    "            images = images.to(device)\n",
    "            multi_labels = torch.stack([encode_evidence_labels(lbl.item()) for lbl in labels]).to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, multi_labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # üéØ Validation accuracy\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            val_correct += (preds == multi_labels.bool()).sum().item()\n",
    "            val_total += torch.numel(multi_labels)\n",
    "\n",
    "    # Validation loss and accuracy for the epoch\n",
    "    val_epoch_loss = val_loss / len(val_loader)\n",
    "    val_epoch_acc = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"‚úÖ Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.2f}%\")\n",
    "    print(f\"üß™ Epoch [{epoch+1}/{EPOCHS}] - Val Loss: {val_epoch_loss:.4f} | Val Acc: {val_epoch_acc:.2f}%\")\n",
    "\n",
    "    # üíæ Save best model based on validation loss\n",
    "    if val_epoch_loss < best_loss:\n",
    "        print(f\"üíæ Validation Loss improved from {best_loss:.4f} to {val_epoch_loss:.4f} - saving model...\")\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        best_loss = val_epoch_loss\n",
    "\n",
    "print(\"\\nüéØ Training completed!\")\n",
    "print(f\"üèÜ Best model saved at: {MODEL_SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fdb5869-64e1-4d18-8f1f-daf41eb99b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded and set to evaluation mode!\n"
     ]
    }
   ],
   "source": [
    "# 1Ô∏è‚É£ Initialize the model architecture first\n",
    "model = VisionTransformer(num_classes=14)  # ‚ö° num_classes must match training\n",
    "\n",
    "# 2Ô∏è‚É£ If you trained using DataParallel, wrap with DataParallel again\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# 3Ô∏è‚É£ Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# 4Ô∏è‚É£ Load saved weights\n",
    "model.load_state_dict(torch.load(\"vit_best_model.pth\", map_location=device))\n",
    "model.eval()  # üí§ VERY IMPORTANT for evaluation\n",
    "print(\"‚úÖ Model loaded and set to evaluation mode!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3de058c7-628d-445a-9ebd-7a859f8f8dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 111308 test samples across 14 classes.\n"
     ]
    }
   ],
   "source": [
    "# ‚ú® Define transform for test set (no augmentation!)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# üóÇÔ∏è Load Test Dataset\n",
    "test_dataset = datasets.ImageFolder(r\"C:/Users/adity/Downloads/Test\", transform=test_transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,  # ‚úÖ IMPORTANT: No shuffle for test\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(test_dataset)} test samples across {len(test_dataset.classes)} classes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58e94cac-3f2c-48e6-89a1-968c0ae81ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Testing:   0%|                                                                             | 0/1740 [00:00<?, ?it/s]C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_24512\\2502406577.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "üîé Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1740/1740 [02:07<00:00, 13.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÅ Test Loss: 0.7845\n",
      "üéØ Test Accuracy: 86.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# üéØ Testing Loop\n",
    "correct = 0\n",
    "total = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "loop = tqdm(test_loader, desc=\"üîé Testing\", leave=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in loop:\n",
    "        images = images.to(device)\n",
    "        multi_labels = torch.stack([encode_evidence_labels(lbl.item()) for lbl in labels]).to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, multi_labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        correct += (preds == multi_labels.bool()).sum().item()\n",
    "        total += torch.numel(multi_labels)\n",
    "\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_acc = 100 * correct / total\n",
    "\n",
    "print(f\"üèÅ Test Loss: {test_loss:.4f}\")\n",
    "print(f\"üéØ Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b6f45-bbeb-489c-a5db-31304245e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Initialize Tkinter\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Hide the root window\n",
    "\n",
    "# Open File Dialog\n",
    "file_path = filedialog.askopenfilename(\n",
    "    title=\"Select an image\",\n",
    "    filetypes=[(\"Image files\", \"*.jpg;*.jpeg;*.png;*.bmp\")]\n",
    ")\n",
    "\n",
    "if not file_path:\n",
    "    print(\"‚ùå No file selected.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nüìÇ Selected file: {file_path}\")\n",
    "\n",
    "# Load Model\n",
    "print(\"\\nüîÑ Loading model...\")\n",
    "progress = tqdm(total=3, bar_format='{l_bar}{bar} | {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')\n",
    "\n",
    "model = VisionTransformer(num_classes=14).to(device)\n",
    "progress.update(1)\n",
    "time.sleep(0.3)\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "progress.update(1)\n",
    "time.sleep(0.3)\n",
    "\n",
    "model.eval()\n",
    "progress.update(1)\n",
    "progress.close()\n",
    "print(\"‚úÖ Model loaded and ready.\\n\")\n",
    "\n",
    "# Image Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# Load and Transform the Uploaded Image\n",
    "print(\"üñºÔ∏è Processing image...\")\n",
    "progress = tqdm(total=2, bar_format='{l_bar}{bar} | {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')\n",
    "\n",
    "image = Image.open(file_path).convert(\"RGB\")\n",
    "progress.update(1)\n",
    "time.sleep(0.3)\n",
    "\n",
    "input_tensor = transform(image).unsqueeze(0).to(device)  # [1, 3, 64, 64]\n",
    "progress.update(1)\n",
    "progress.close()\n",
    "print(\"‚úÖ Image ready.\\n\")\n",
    "\n",
    "# Predict\n",
    "print(\"üîç Predicting...\")\n",
    "with torch.no_grad(), torch.amp.autocast(device_type=\"cuda\"):\n",
    "    outputs = model(input_tensor)\n",
    "    preds = torch.sigmoid(outputs) > 0.5  # multi-label threshold\n",
    "\n",
    "print(\"üéØ Prediction Complete!\\n\")\n",
    "print(f\"‚úÖ Prediction Result: {preds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65b900-2143-42cf-a676-7a1222d45ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
