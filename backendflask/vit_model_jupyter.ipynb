{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "929cab58-76d7-4506-89d9-9f3138acc647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported and seeds set!\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Install missing packages if any\n",
    "# !pip install -q torch torchvision tqdm\n",
    "\n",
    "# 📦 Imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 🧹 Set random seeds for reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "print(\"✅ Libraries imported and seeds set!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "382ae0cd-3409-4de3-bd34-d4cf9fa838e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evidence classes and label encoder ready!\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Evidence classes\n",
    "evidence_classes = [\n",
    "    \"Gun\", \"Knife\", \"Mask\", \"Car\", \"Fire\", \"Glass\", \"Crowd\",\n",
    "    \"Blood\", \"Explosion\", \"Bag\", \"Money\", \"Weapon\", \"Smoke\", \"Person\"\n",
    "]\n",
    "\n",
    "# 🔗 Crime-to-evidence mapping\n",
    "crime_to_evidence = {\n",
    "    \"Arrest\": [\"Person\", \"Crowd\"],\n",
    "    \"Arson\": [\"Fire\", \"Smoke\"],\n",
    "    \"Assault\": [\"Blood\", \"Person\"],\n",
    "    \"Burglary\": [\"Bag\", \"Glass\"],\n",
    "    \"Explosion\": [\"Explosion\", \"Smoke\"],\n",
    "    \"Fighting\": [\"Crowd\", \"Blood\"],\n",
    "    \"RoadAccidents\": [\"Car\", \"Person\"],\n",
    "    \"Robbery\": [\"Gun\", \"Bag\", \"Mask\"],\n",
    "    \"Shooting\": [\"Gun\", \"Crowd\"],\n",
    "    \"Shoplifting\": [\"Bag\", \"Money\"],\n",
    "    \"Stealing\": [\"Bag\", \"Person\"],\n",
    "    \"Vandalism\": [\"Glass\", \"Crowd\"],\n",
    "    \"Abuse\": [\"Person\"],\n",
    "    \"NormalVideos\": []\n",
    "}\n",
    "\n",
    "# 🏷️ Encode function\n",
    "def encode_evidence_labels(crime_label_idx):\n",
    "    index_to_class = {\n",
    "        0: \"Arrest\", 1: \"Arson\", 2: \"Assault\", 3: \"Burglary\", 4: \"Explosion\",\n",
    "        5: \"Fighting\", 6: \"RoadAccidents\", 7: \"Robbery\", 8: \"Shooting\",\n",
    "        9: \"Shoplifting\", 10: \"Stealing\", 11: \"Vandalism\", 12: \"Abuse\", 13: \"NormalVideos\"\n",
    "    }\n",
    "    crime_class = index_to_class[crime_label_idx]\n",
    "    evidences = crime_to_evidence.get(crime_class, [])\n",
    "    binary = [1 if ev in evidences else 0 for ev in evidence_classes]\n",
    "    return torch.tensor(binary, dtype=torch.float32)\n",
    "\n",
    "print(\"✅ Evidence classes and label encoder ready!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119b348f-411f-4423-a617-c770487208a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vision Transformer model defined!\n"
     ]
    }
   ],
   "source": [
    "# 🧠 Vision Transformer (ViT) for Evidence Detection\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=14, dim=256, depth=6, heads=8, mlp_dim=512, patch_size=8, image_size=64):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.dim = dim\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.conv = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, dim))\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, batch_first=True),\n",
    "            num_layers=depth\n",
    "        )\n",
    "\n",
    "        self.to_evidence = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)                  # (B, D, 8, 8)\n",
    "        x = x.flatten(2).transpose(1, 2)   # (B, 64, D)\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)  # (B, 1, D)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)                   # (B, 65, D)\n",
    "        x = x + self.pos_embedding\n",
    "        x = self.transformer(x)\n",
    "        x = x[:, 0]  # CLS token output\n",
    "        return self.to_evidence(x)\n",
    "\n",
    "print(\"✅ Vision Transformer model defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a87e1a2c-a3d2-4722-9930-8941b9fee127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Setting up data transformations...\n",
      "✅ Transformations set!\n",
      "⚙️ Batch Size: 128, Number of Classes: 14\n",
      "🔥 Device selected: cuda\n",
      "📂 Loading UCF Crime dataset...\n",
      "📊 Dataset contains 1266345 samples across 14 classes.\n",
      "📚 Classes detected: ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fighting', 'NormalVideos', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']\n",
      "✅ DataLoader is ready!\n",
      "🚀 Everything ready to start training!\n"
     ]
    }
   ],
   "source": [
    "# 📂 Define Transformations\n",
    "print(\"🔧 Setting up data transformations...\")\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "print(\"✅ Transformations set!\")\n",
    "\n",
    "# 🔥 Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = 14\n",
    "print(f\"⚙️ Batch Size: {BATCH_SIZE}, Number of Classes: {NUM_CLASSES}\")\n",
    "\n",
    "# 🚀 Set Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🔥 Device selected: {device}\")\n",
    "\n",
    "# 🗂️ Load Dataset\n",
    "print(\"📂 Loading UCF Crime dataset...\")\n",
    "train_dataset = datasets.ImageFolder(r\"C:/Users/adity/Downloads/Train\", transform=train_transform)\n",
    "\n",
    "print(f\"📊 Dataset contains {len(train_dataset)} samples across {len(train_dataset.classes)} classes.\")\n",
    "print(f\"📚 Classes detected: {train_dataset.classes}\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,   # Kaggle usually gives 2 CPUs\n",
    "    pin_memory=True\n",
    ")\n",
    "print(\"✅ DataLoader is ready!\")\n",
    "print(\"🚀 Everything ready to start training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e690292e-516e-42d3-852a-63332c932148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model, Loss, Optimizer ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_24512\\2226223630.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# 🧠 Model\n",
    "model = VisionTransformer(num_classes=NUM_CLASSES)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"🚀 Using {torch.cuda.device_count()} GPUs with DataParallel!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# 🎯 Loss and Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "print(\"✅ Model, Loss, Optimizer ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d576468d-2b58-41f3-9e71-8db7d26e0abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 Split: 1139711 training samples and 126634 validation samples.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# 🛠️ Split into train and validation\n",
    "VAL_SPLIT = 0.1  # 10% for validation\n",
    "\n",
    "# Calculate lengths\n",
    "num_samples = len(train_dataset)\n",
    "num_val = int(VAL_SPLIT * num_samples)\n",
    "num_train = num_samples - num_val\n",
    "\n",
    "# Random split\n",
    "train_dataset, val_dataset = random_split(train_dataset, [num_train, num_val])\n",
    "print(f\"🧩 Split: {num_train} training samples and {num_val} validation samples.\")\n",
    "\n",
    "# Reload loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03e65d8c-6357-4355-b49a-c7b21c1ea826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] (Training): 100%|███████████████████████████████████████| 8904/8904 [15:58<00:00,  9.29it/s, loss=0.00374]\n",
      "Epoch [1/10] (Validation): 100%|█████████████████████████████████████████████████████| 990/990 [01:52<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [1/10] - Train Loss: 0.0109 | Train Acc: 99.64%\n",
      "🧪 Epoch [1/10] - Val Loss: 0.0052 | Val Acc: 99.83%\n",
      "💾 Validation Loss improved from 0.0352 to 0.0052 - saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] (Training): 100%|██████████████████████████████████████| 8904/8904 [15:09<00:00,  9.79it/s, loss=0.000848]\n",
      "Epoch [2/10] (Validation): 100%|█████████████████████████████████████████████████████| 990/990 [01:46<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [2/10] - Train Loss: 0.0070 | Train Acc: 99.76%\n",
      "🧪 Epoch [2/10] - Val Loss: 0.0036 | Val Acc: 99.88%\n",
      "💾 Validation Loss improved from 0.0052 to 0.0036 - saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] (Training): 100%|███████████████████████████████████████| 8904/8904 [15:35<00:00,  9.52it/s, loss=0.00411]\n",
      "Epoch [3/10] (Validation): 100%|█████████████████████████████████████████████████████| 990/990 [01:59<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [3/10] - Train Loss: 0.0053 | Train Acc: 99.82%\n",
      "🧪 Epoch [3/10] - Val Loss: 0.0029 | Val Acc: 99.90%\n",
      "💾 Validation Loss improved from 0.0036 to 0.0029 - saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] (Training): 100%|███████████████████████████████████████| 8904/8904 [14:18<00:00, 10.38it/s, loss=0.00117]\n",
      "Epoch [4/10] (Validation): 100%|█████████████████████████████████████████████████████| 990/990 [01:36<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [4/10] - Train Loss: 0.0045 | Train Acc: 99.85%\n",
      "🧪 Epoch [4/10] - Val Loss: 0.0035 | Val Acc: 99.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] (Training): 100%|██████████████████████████████████████| 8904/8904 [15:09<00:00,  9.79it/s, loss=0.000604]\n",
      "Epoch [5/10] (Validation): 100%|█████████████████████████████████████████████████████| 990/990 [01:57<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [5/10] - Train Loss: 0.0039 | Train Acc: 99.87%\n",
      "🧪 Epoch [5/10] - Val Loss: 0.0026 | Val Acc: 99.91%\n",
      "💾 Validation Loss improved from 0.0029 to 0.0026 - saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] (Training): 100%|███████████████████████████████████████| 8904/8904 [16:55<00:00,  8.77it/s, loss=0.00019]\n",
      "Epoch [6/10] (Validation): 100%|█████████████████████████████████████████████████████| 990/990 [02:01<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [6/10] - Train Loss: 0.0035 | Train Acc: 99.88%\n",
      "🧪 Epoch [6/10] - Val Loss: 0.0023 | Val Acc: 99.92%\n",
      "💾 Validation Loss improved from 0.0026 to 0.0023 - saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] (Training): 100%|██████████████████████████████████████| 8904/8904 [17:29<00:00,  8.49it/s, loss=0.000701]\n",
      "Epoch [7/10] (Validation): 100%|█████████████████████████████████████████████████████| 990/990 [01:57<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [7/10] - Train Loss: 0.0032 | Train Acc: 99.89%\n",
      "🧪 Epoch [7/10] - Val Loss: 0.0022 | Val Acc: 99.93%\n",
      "💾 Validation Loss improved from 0.0023 to 0.0022 - saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] (Training): 100%|███████████████████████████████████████| 8904/8904 [16:26<00:00,  9.03it/s, loss=0.00135]\n",
      "Epoch [8/10] (Validation): 100%|█████████████████████████████████████████████████████| 990/990 [01:55<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [8/10] - Train Loss: 0.0029 | Train Acc: 99.90%\n",
      "🧪 Epoch [8/10] - Val Loss: 0.0019 | Val Acc: 99.94%\n",
      "💾 Validation Loss improved from 0.0022 to 0.0019 - saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] (Training): 100%|██████████████████████████████████████| 8904/8904 [16:10<00:00,  9.17it/s, loss=0.000128]\n",
      "Epoch [9/10] (Validation): 100%|█████████████████████████████████████████████████████| 990/990 [01:53<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [9/10] - Train Loss: 0.0028 | Train Acc: 99.90%\n",
      "🧪 Epoch [9/10] - Val Loss: 0.0022 | Val Acc: 99.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] (Training): 100%|██████████████████████████████████████| 8904/8904 [16:34<00:00,  8.95it/s, loss=0.00612]\n",
      "Epoch [10/10] (Validation): 100%|████████████████████████████████████████████████████| 990/990 [01:57<00:00,  8.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [10/10] - Train Loss: 0.0026 | Train Acc: 99.91%\n",
      "🧪 Epoch [10/10] - Val Loss: 0.0020 | Val Acc: 99.94%\n",
      "\n",
      "🎯 Training completed!\n",
      "🏆 Best model saved at: vit_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"🚀 Starting training...\\n\")\n",
    "\n",
    "# Ensure optimizer is linked to model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{EPOCHS}] (Training)\", leave=True)\n",
    "\n",
    "    for images, labels in loop:\n",
    "        images = images.to(device)\n",
    "        multi_labels = torch.stack([encode_evidence_labels(lbl.item()) for lbl in labels]).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, multi_labels)\n",
    "\n",
    "        # Backpropagate gradients with scaled loss\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        try:\n",
    "            scaler.step(optimizer)  # Step optimizer only if gradients are not missing\n",
    "            scaler.update()\n",
    "        except AssertionError:\n",
    "            print(\"⚠️ Skipping optimizer step due to missing gradients.\")\n",
    "            continue  # Skip this batch if gradients are missing\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 🎯 Calculate batch accuracy\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        correct += (preds == multi_labels.bool()).sum().item()\n",
    "        total += torch.numel(multi_labels)\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Average loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "\n",
    "    # 🧪 Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_total = 0\n",
    "    val_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loop = tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{EPOCHS}] (Validation)\", leave=True)\n",
    "\n",
    "        for images, labels in val_loop:\n",
    "            images = images.to(device)\n",
    "            multi_labels = torch.stack([encode_evidence_labels(lbl.item()) for lbl in labels]).to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, multi_labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # 🎯 Validation accuracy\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            val_correct += (preds == multi_labels.bool()).sum().item()\n",
    "            val_total += torch.numel(multi_labels)\n",
    "\n",
    "    # Validation loss and accuracy for the epoch\n",
    "    val_epoch_loss = val_loss / len(val_loader)\n",
    "    val_epoch_acc = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"✅ Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.2f}%\")\n",
    "    print(f\"🧪 Epoch [{epoch+1}/{EPOCHS}] - Val Loss: {val_epoch_loss:.4f} | Val Acc: {val_epoch_acc:.2f}%\")\n",
    "\n",
    "    # 💾 Save best model based on validation loss\n",
    "    if val_epoch_loss < best_loss:\n",
    "        print(f\"💾 Validation Loss improved from {best_loss:.4f} to {val_epoch_loss:.4f} - saving model...\")\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        best_loss = val_epoch_loss\n",
    "\n",
    "print(\"\\n🎯 Training completed!\")\n",
    "print(f\"🏆 Best model saved at: {MODEL_SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fdb5869-64e1-4d18-8f1f-daf41eb99b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded and set to evaluation mode!\n"
     ]
    }
   ],
   "source": [
    "# 1️⃣ Initialize the model architecture first\n",
    "model = VisionTransformer(num_classes=14)  # ⚡ num_classes must match training\n",
    "\n",
    "# 2️⃣ If you trained using DataParallel, wrap with DataParallel again\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# 3️⃣ Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# 4️⃣ Load saved weights\n",
    "model.load_state_dict(torch.load(\"vit_best_model.pth\", map_location=device))\n",
    "model.eval()  # 💤 VERY IMPORTANT for evaluation\n",
    "print(\"✅ Model loaded and set to evaluation mode!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3de058c7-628d-445a-9ebd-7a859f8f8dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 111308 test samples across 14 classes.\n"
     ]
    }
   ],
   "source": [
    "# ✨ Define transform for test set (no augmentation!)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# 🗂️ Load Test Dataset\n",
    "test_dataset = datasets.ImageFolder(r\"C:/Users/adity/Downloads/Test\", transform=test_transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,  # ✅ IMPORTANT: No shuffle for test\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Loaded {len(test_dataset)} test samples across {len(test_dataset.classes)} classes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58e94cac-3f2c-48e6-89a1-968c0ae81ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Testing:   0%|                                                                             | 0/1740 [00:00<?, ?it/s]C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_24512\\2502406577.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "🔎 Testing: 100%|██████████████████████████████████████████████████████████████████| 1740/1740 [02:07<00:00, 13.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏁 Test Loss: 0.7845\n",
      "🎯 Test Accuracy: 86.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Testing Loop\n",
    "correct = 0\n",
    "total = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "loop = tqdm(test_loader, desc=\"🔎 Testing\", leave=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in loop:\n",
    "        images = images.to(device)\n",
    "        multi_labels = torch.stack([encode_evidence_labels(lbl.item()) for lbl in labels]).to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, multi_labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        correct += (preds == multi_labels.bool()).sum().item()\n",
    "        total += torch.numel(multi_labels)\n",
    "\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_acc = 100 * correct / total\n",
    "\n",
    "print(f\"🏁 Test Loss: {test_loss:.4f}\")\n",
    "print(f\"🎯 Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b6f45-bbeb-489c-a5db-31304245e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Initialize Tkinter\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Hide the root window\n",
    "\n",
    "# Open File Dialog\n",
    "file_path = filedialog.askopenfilename(\n",
    "    title=\"Select an image\",\n",
    "    filetypes=[(\"Image files\", \"*.jpg;*.jpeg;*.png;*.bmp\")]\n",
    ")\n",
    "\n",
    "if not file_path:\n",
    "    print(\"❌ No file selected.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n📂 Selected file: {file_path}\")\n",
    "\n",
    "# Load Model\n",
    "print(\"\\n🔄 Loading model...\")\n",
    "progress = tqdm(total=3, bar_format='{l_bar}{bar} | {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')\n",
    "\n",
    "model = VisionTransformer(num_classes=14).to(device)\n",
    "progress.update(1)\n",
    "time.sleep(0.3)\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "progress.update(1)\n",
    "time.sleep(0.3)\n",
    "\n",
    "model.eval()\n",
    "progress.update(1)\n",
    "progress.close()\n",
    "print(\"✅ Model loaded and ready.\\n\")\n",
    "\n",
    "# Image Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# Load and Transform the Uploaded Image\n",
    "print(\"🖼️ Processing image...\")\n",
    "progress = tqdm(total=2, bar_format='{l_bar}{bar} | {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')\n",
    "\n",
    "image = Image.open(file_path).convert(\"RGB\")\n",
    "progress.update(1)\n",
    "time.sleep(0.3)\n",
    "\n",
    "input_tensor = transform(image).unsqueeze(0).to(device)  # [1, 3, 64, 64]\n",
    "progress.update(1)\n",
    "progress.close()\n",
    "print(\"✅ Image ready.\\n\")\n",
    "\n",
    "# Predict\n",
    "print(\"🔍 Predicting...\")\n",
    "with torch.no_grad(), torch.amp.autocast(device_type=\"cuda\"):\n",
    "    outputs = model(input_tensor)\n",
    "    preds = torch.sigmoid(outputs) > 0.5  # multi-label threshold\n",
    "\n",
    "print(\"🎯 Prediction Complete!\\n\")\n",
    "print(f\"✅ Prediction Result: {preds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65b900-2143-42cf-a676-7a1222d45ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
